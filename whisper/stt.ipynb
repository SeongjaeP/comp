{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     AutoQuantizationConfig,\n\u001b[1;32m      4\u001b[0m     ORTModelForSpeechSeq2Seq,\n\u001b[1;32m      5\u001b[0m     ORTQuantizer\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Configure base model and save directory for compressed model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-large-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optimum'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from optimum.onnxruntime import (\n",
    "    AutoQuantizationConfig,\n",
    "    ORTModelForSpeechSeq2Seq,\n",
    "    ORTQuantizer\n",
    ")\n",
    "\n",
    "# Configure base model and save directory for compressed model\n",
    "model_id = \"openai/whisper-large-v2\"\n",
    "save_dir = \"whisper-large\"\n",
    "\n",
    "# Export model in ONNX\n",
    "model = ORTModelForSpeechSeq2Seq.from_pretrained(model_id, export=True)\n",
    "model_dir = model.model_save_dir\n",
    "\n",
    "# Run quantization for all ONNX files of exported model\n",
    "onnx_models = list(Path(model_dir).glob(\"*.onnx\"))\n",
    "print(onnx_models)\n",
    "quantizers = [ORTQuantizer.from_pretrained(model_dir, file_name=onnx_model) for onnx_model in onnx_models]\n",
    "\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "\n",
    "for quantizer in quantizers:\n",
    "    # Apply dynamic quantization and save the resulting model\n",
    "    quantizer.quantize(save_dir=save_dir, quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ground_truth 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 추출된 Ground Truth 리스트:\n",
      "119니다.\n",
      "어 네, 여기 저기 어디지? 고속더미널.\n",
      "네.\n",
      "그 맞은편 육교에 있는 육교 다리 밑인데요.\n",
      "선생님, 그렇게 말하면 저희가 악기, 알기가 어려우니까 고속터미널에 해서 그 육교가 어느 방향 쪽이예요? 거기 근처에 주소 아세요?\n",
      "네.\n",
      "네.\n",
      "네.\n",
      "아, 고속터미널, 그 지하철역 3출구 앞에 있는 육교거든요.\n",
      "3번 출구요?\n",
      "네, 네, 네.\n",
      "고속터미널 3호선 3번 출구?\n",
      "네네네. 그.\n",
      "아, 7호선 있구나, 그쪽이.\n",
      "예, 7호선 3호선 다 다녀요, 그니까 어쨌든 3번 출구 7호선일 거예요, 아마.\n",
      "네, 7호선 3호 출구, 그 옆에 스타벅스 있고, 그런데 쪽 말하는 거죠?\n",
      "네.\n",
      "네, 맞아요, 맞아요, 맞아요.\n",
      "예, 그쪽 앞에 육교 쪽에, 네.\n",
      "거기 육교, 다리 밑인데 여기 남성 한 분이 쓰러져 계시는데요.\n",
      "네, 그, 한번.\n",
      "그 경련도 좀 하시는 거 같고.\n",
      "경련을 하세요?\n",
      "예, 의식은 있으신데, 네.\n",
      "의식은 있고 호흡도 있고 그분이 막 부들 떨고 있어요? 바들바들?\n",
      "네, 호흡은 있는데.\n",
      "네네, 몸을 떨고 계시구요. 지금 약간 배설물도 나오시는 거 같거든요\n",
      "예, 그래요? 선생님, 그쪽 근처에 한번 가보시고.\n",
      "네, 네.\n",
      "네.\n",
      "저희가 의료지도도 해드릴 거니깐 전화는 끊지 말구요. 잠시만요.\n",
      "네, 네.\n",
      "지금 구급차는 나갔어요. 저희가 해드릴 거니까 잠시만 기다려주세요.\n",
      "네.\n",
      "네.\n",
      "네.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1️⃣ JSON 파일 로드\n",
    "json_path = \"dataset/Sample/02.라벨링데이터/서울/구급/651e464d69a4f266f06268a0_20220101.json\"  # JSON 파일 경로\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2️⃣ \"text\" 값만 추출하여 `ground_truth` 리스트 생성\n",
    "ground_truth = [utterance[\"text\"] for utterance in data[\"utterances\"]]\n",
    "\n",
    "# 3️⃣ 결과 출력\n",
    "print(\"\\n✅ 추출된 Ground Truth 리스트:\")\n",
    "for text in ground_truth:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/whisper/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Whisper 변환 결과:\n",
      " 119입니다. 어 네 여기 저기 어디지? 고속터미널 맞은편 6교에 있는 6교 발입 위치인데요. 네 그렇게 말하면 저희가 알기가 어려우니까 고속터미널에서 그 6교가 어느 방향 쪽이에요? 거기 근처에 주소 아세요? 아 고속터미널 그 지하철역 3번 출구 앞에 있는 6교거든요. 3번 출구요? 네 네 네. 고속터미널 3호선 3번 출구? 아 7호선이구나 그쪽이. 네 7호선 3호선 다 다녀요. 그니까 어쨌든 3번 출구 7호선일 거예요 아마. 네 7호선 3번 출구 그 옆에 스타벅스 있고 그런 데 쪽 말하는 거죠? 네 맞아요 맞아요 맞아요. 네 그쪽 아시겠죠? 네 네. 발이 밑인데 여기 남동 1분이 쓰러져 계시는데요. 네 그 1번. 정렴도 좀 하시는 거 같고. 정리 안 해도 돼요? 네 의식은 있으신데 네. 의식은 있고 호흡도 있고 그분이 막 끌고 있어요? 네 네 몸을 떨고 계시구요. 지금 약간 배설물도 나오시는 거 같거든요. 네 그래요? 선생님 그쪽 근처로 1번 가 보시고 저희가 의료지도도 해드릴 거니까 전화는 끊지 말고요. 잠시만요. 네 네. 지금 구급차는 나갔어요. 저희가 드릴 거니까 잠시만 기다려주세요.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# 1️⃣ 모델 로드 (small, medium, large 선택 가능)\n",
    "model = whisper.load_model(\"large\")  # \"small\", \"medium\", \"large\" 중 선택 가능\n",
    "\n",
    "# 2️⃣ 음성 파일 변환\n",
    "audio_path = \"dataset/Sample/01.원천데이터/서울/구급/651e464d69a4f266f06268a0_20220101.wav\"  # 변환할 오디오 파일 경로\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "# 3️⃣ 변환된 텍스트 출력\n",
    "print(\"\\n✅ Whisper 변환 결과:\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from jiwer import wer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"WER 계산을 위해 ground_truth와 whisper_text를 동일한 형식으로 변환하는 함수\"\"\"\n",
    "    text = text.lower()  # 대소문자 통일\n",
    "    text = text.replace(\"\\n\", \" \")  # 줄바꿈을 띄어쓰기로 변경\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 특수문자 제거\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 불필요한 공백 제거\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_text = result[\"text\"]\n",
    "ground_truth_str = \" \".join(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mground_truth_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'dtypes'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_cleaned = preprocess_text(ground_truth_str)\n",
    "whisper_text_cleaned = preprocess_text(whisper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'119입니다 어 네 여기 저기 어디지 고속터미널 맞은편 6교에 있는 6교 발입 위치인데요 네 그렇게 말하면 저희가 알기가 어려우니까 고속터미널에서 그 6교가 어느 방향 쪽이에요 거기 근처에 주소 아세요 아 고속터미널 그 지하철역 3번 출구 앞에 있는 6교거든요 3번 출구요 네 네 네 고속터미널 3호선 3번 출구 아 7호선이구나 그쪽이 네 7호선 3호선 다 다녀요 그니까 어쨌든 3번 출구 7호선일 거예요 아마 네 7호선 3번 출구 그 옆에 스타벅스 있고 그런 데 쪽 말하는 거죠 네 맞아요 맞아요 맞아요 네 그쪽 아시겠죠 네 네 발이 밑인데 여기 남동 1분이 쓰러져 계시는데요 네 그 1번 정렴도 좀 하시는 거 같고 정리 안 해도 돼요 네 의식은 있으신데 네 의식은 있고 호흡도 있고 그분이 막 끌고 있어요 네 네 몸을 떨고 계시구요 지금 약간 배설물도 나오시는 거 같거든요 네 그래요 선생님 그쪽 근처로 1번 가 보시고 저희가 의료지도도 해드릴 거니까 전화는 끊지 말고요 잠시만요 네 네 지금 구급차는 나갔어요 저희가 드릴 거니까 잠시만 기다려주세요'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3815028901734104\n",
      "\n",
      " WER (Word Error Rate): 38.15%\n"
     ]
    }
   ],
   "source": [
    "wer_score =wer(ground_truth_cleaned, whisper_text_cleaned)\n",
    "print(wer_score)\n",
    "print(f\"\\n WER (Word Error Rate): {wer_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 119입니다. 어 네 여기 저기 어디지? 고속터미널 맞은편 6교에 있는 6교 발입 위치인데요. 네 그렇게 말하면 저희가 알기가 어려우니까 고속터미널에서 그 6교가 어느 방향 쪽이에요? 거기 근처에 주소 아세요? 아 고속터미널 그 지하철역 3번 출구 앞에 있는 6교거든요. 3번 출구요? 네 네 네. 고속터미널 3호선 3번 출구? 아 7호선이구나 그쪽이. 네 7호선 3호선 다 다녀요. 그니까 어쨌든 3번 출구 7호선일 거예요 아마. 네 7호선 3번 출구 그 옆에 스타벅스 있고 그런 데 쪽 말하는 거죠? 네 맞아요 맞아요 맞아요. 네 그쪽 아시겠죠? 네 네. 발이 밑인데 여기 남동 1분이 쓰러져 계시는데요. 네 그 1번. 정렴도 좀 하시는 거 같고. 정리 안 해도 돼요? 네 의식은 있으신데 네. 의식은 있고 호흡도 있고 그분이 막 끌고 있어요? 네 네 몸을 떨고 계시구요. 지금 약간 배설물도 나오시는 거 같거든요. 네 그래요? 선생님 그쪽 근처로 1번 가 보시고 저희가 의료지도도 해드릴 거니까 전화는 끊지 말고요. 잠시만요. 네 네. 지금 구급차는 나갔어요. 저희가 드릴 거니까 잠시만 기다려주세요.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## faster whisper model - large by quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(7-1) Make user-defined functions for STT\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[?.])\\s*', text)\n",
    "    return sentences\n",
    "\n",
    "def extract_text_from_file(file_path: str) -> dict :\n",
    "    model_size = \"large-v3\"\n",
    "    model = WhisperModel(model_size, device='auto', compute_type = \"int8\")\n",
    "    segments, info = model.transcribe(\n",
    "        file_path,\n",
    "        language=\"ko\",\n",
    "        beam_size=10,\n",
    "        word_timestamps=True,\n",
    "        vad_filter=True,\n",
    "        vad_parameters=dict(min_silence_duration_ms=500)\n",
    "    )\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## whisper model - quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/whisper/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Whisper 변환 결과 (INT8 적용):\n",
      " 119입니다. 어 네 여기 저기 어디지? 고속터미널 맞은편 6교에 있는 6교 발입 위치인데요. 네 그렇게 말하면 저희가 알기가 어려우니까 고속터미널에서 그 6교가 어느 방향 쪽이에요? 거기 근처에 주소 아세요? 아 고속터미널 그 지하철역 3번 출구 앞에 있는 6교거든요. 3번 출구요? 네 네 네. 고속터미널 3호선 3번 출구? 아 7호선이구나 그쪽이. 네 7호선 3호선 다 다녀요. 그니까 어쨌든 3번 출구 7호선일 거예요 아마. 네 7호선 3번 출구 그 옆에 스타벅스 있고 그런 데 쪽 말하는 거죠? 네 맞아요 맞아요 맞아요. 네 그쪽 아시겠죠? 네 네. 발이 밑인데 여기 남동 1분이 쓰러져 계시는데요. 네 그 1번. 정렴도 좀 하시는 거 같고. 정리 안 해도 돼요? 네 의식은 있으신데 네. 의식은 있고 호흡도 있고 그분이 막 끌고 있어요? 네 네 몸을 떨고 계시구요. 지금 약간 배설물도 나오시는 거 같거든요. 네 그래요? 선생님 그쪽 근처로 1번 가 보시고 저희가 의료지도도 해드릴 거니까 전화는 끊지 말고요. 잠시만요. 네 네. 지금 구급차는 나갔어요. 저희가 드릴 거니까 잠시만 기다려주세요.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "\n",
    "# ✅ 1️⃣ 모델 로드\n",
    "model = whisper.load_model(\"large\")\n",
    "\n",
    "# ✅ 2️⃣ Quantization 적용 (INT8)\n",
    "model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# ✅ 3️⃣ 음성 파일 변환\n",
    "audio_path = \"dataset/Sample/01.원천데이터/서울/구급/651e464d69a4f266f06268a0_20220101.wav\"\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "# ✅ 4️⃣ 변환된 텍스트 출력\n",
    "print(\"\\n✅ Whisper 변환 결과 (INT8 적용):\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 현재 사용 중인 디바이스: mps\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseMPS' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_sparse_coo_tensor_with_dims_and_tensors' is only available for these backends: [MPS, Meta, SparseCPU, SparseMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:78 [backend fallback]\nMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta.cpp:27006 [kernel]\nSparseCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1407 [kernel]\nSparseMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseMeta.cpp:291 [kernel]\nBackendSelect: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:792 [kernel]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:503 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:17801 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\nAutocastXPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:465 [backend fallback]\nAutocastMPS: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:499 [backend fallback]\nPreDispatch: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 현재 사용 중인 디바이스: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ✅ Whisper 모델 로드 (FP16 적용)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlarge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# FP16 적용\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ✅ 음성 파일 변환\u001b[39;00m\n\u001b[1;32m     12\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/Sample/01.원천데이터/서울/구급/651e464d69a4f266f06268a0_20220101.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/whisper/lib/python3.11/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/whisper/lib/python3.11/site-packages/torch/nn/modules/module.py:991\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 991\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/whisper/lib/python3.11/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseMPS' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_sparse_coo_tensor_with_dims_and_tensors' is only available for these backends: [MPS, Meta, SparseCPU, SparseMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:78 [backend fallback]\nMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta.cpp:27006 [kernel]\nSparseCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1407 [kernel]\nSparseMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseMeta.cpp:291 [kernel]\nBackendSelect: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:792 [kernel]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:503 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:17801 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\nAutocastXPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:465 [backend fallback]\nAutocastMPS: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:499 [backend fallback]\nPreDispatch: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "# ✅ Whisper 모델 로드 (기본 FP32)\n",
    "model = whisper.load_model(\"large\")\n",
    "\n",
    "# ✅ ONNX로 변환하여 CPU에서 최적화 실행\n",
    "dummy_input = torch.randn(1, 80, 3000)\n",
    "torch.onnx.export(model, dummy_input, \"whisper_model.onnx\")\n",
    "\n",
    "# ✅ ONNX 모델 실행\n",
    "session = ort.InferenceSession(\"whisper_model.onnx\")\n",
    "audio_path = \"dataset/Sample/01.원천데이터/서울/구급/651e464d69a4f266f06268a0_20220101.wav\"\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "# ✅ 변환된 텍스트 출력\n",
    "print(\"\\n✅ Whisper 변환 결과 (ONNX 최적화 적용):\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 현재 사용 중인 디바이스: mps\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
